---
title: "XGBoost"
author: "Manuel Rocamora Valenti"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(caret)
library(xgboost)
library(openxlsx)
library(xgboost)

```

Cargamos los datos

```{r}
data <- read.xlsx("datos_PCA.xlsx")
```

Preparamos los datos

```{r}
# Y debe ser un factor con las clases
Y <- as.factor(data$`X1ª_eval`)

# X debe ser un data.frame solo con variables numéricas
X <- data[, !(names(data) %in% c("X1ª_eval", "Mejor_resp"))]  # quitar variables respuesta
X <- X[, sapply(X, is.numeric)]  # filtrar numéricas
X <- as.data.frame(X)  # asegurar que es data.frame

# Convertir a matriz y asegurarte de que los nombres sean válidos
X_matrix <- as.matrix(X)
colnames(X_matrix) <- make.names(colnames(X), unique = TRUE)

```

Definir y entrenas modelo XGBoot

```{r}
library(xgboost)
library(caret)

# Matriz y etiquetas (deben estar definidos ya)
# X_matrix: matriz numérica con nombres válidos
# Y: factor con etiquetas
Y_num <- as.numeric(Y) - 1
n <- nrow(X_matrix)
preds <- numeric(n)

# Loop Leave-One-Out
for (i in 1:n) {
  train_idx <- setdiff(1:n, i)
  dtrain <- xgb.DMatrix(data = X_matrix[train_idx, ], label = Y_num[train_idx])
  dtest  <- xgb.DMatrix(data = X_matrix[i, , drop = FALSE])

  model <- xgboost(
    params = list(
      objective = "multi:softprob",
      num_class = length(unique(Y_num)),
      eval_metric = "mlogloss",
      max_depth = 3,
      eta = 0.1,
      verbosity = 0
    ),
    data = dtrain,
    nrounds = 50,
    verbose = 0
  )

  prob <- predict(model, newdata = dtest)
  pred_class <- which.max(prob) - 1
  preds[i] <- pred_class
}

# Evaluar rendimiento
pred_factor <- factor(preds, levels = 0:(length(levels(Y)) - 1), labels = levels(Y))
confusionMatrix(pred_factor, Y)
```

Evaluacion

```{r}
# Entrena un modelo completo (sin LOO) solo para obtener importancia
dtrain_full <- xgb.DMatrix(data = X_matrix, label = Y_num)

params <- list(
  objective = "multi:softprob",
  num_class = length(unique(Y_num)),
  eval_metric = "mlogloss",
  max_depth = 3,
  eta = 0.1
)

xgb_model_full <- xgboost(
  params = params,
  data = dtrain_full,
  nrounds = 50,
  verbose = 0
)

# Extraer importancia de variables
importance <- xgb.importance(model = xgb_model_full)

# Mostrar top 10
print(importance[1:10, ])



# Escoge top 10 variables más importantes
top_vars <- importance$Feature[1:10]  # o 15 o 20 si prefieres

# Crear nuevo X reducido
X_top <- X[, top_vars, drop = FALSE]
X_top_matrix <- as.matrix(X_top)

n <- nrow(X_top_matrix)
preds <- numeric(n)

for (i in 1:n) {
  train_idx <- setdiff(1:n, i)
  dtrain <- xgb.DMatrix(data = X_top_matrix[train_idx, ], label = Y_num[train_idx])
  dtest  <- xgb.DMatrix(data = X_top_matrix[i, , drop = FALSE])

  model <- xgboost(
    params = params,
    data = dtrain,
    nrounds = 50,
    verbose = 0
  )

  prob <- predict(model, newdata = dtest)
  pred_class <- which.max(prob) - 1
  preds[i] <- pred_class
}

# Evaluar
pred_factor <- factor(preds, levels = 0:(length(levels(Y)) - 1), labels = levels(Y))
confusionMatrix(pred_factor, Y)
```

Tras entrenar un modelo XGBoost multiclase con validación Leave-One-Out, se observó un bajo rendimiento cuando se utilizaron todas las variables. Sin embargo, al restringir el entrenamiento a las 10 variables más importantes (según la ganancia acumulada del modelo), se obtuvo una mejora significativa en la capacidad predictiva (accuracy: 55.9%, kappa: 0.27). Esto demuestra que la selección previa de características es especialmente relevante en problemas con p \>\> n, como en este estudio.\

\
