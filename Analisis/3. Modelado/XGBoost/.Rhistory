# 3. ENTRENAR MODELO FINAL PARA SHAP
#---------------------------------------------
dtrain_top <- xgb.DMatrix(data = X_matrix_named, label = Y_num)
xgb_model_top <- xgboost(
params = params,
data = dtrain_top,
nrounds = 50,
verbose = 0
)
#---------------------------------------------
# 4. CALCULAR Y VISUALIZAR SHAP
#---------------------------------------------
shap_values <- shap.values(xgb_model = xgb_model_top, X_train = X_matrix_named)
dim(X_matrix_named)
# Cargar librerías necesarias
library(xgboost)
library(SHAPforxgboost)
library(dplyr)
#---------------------------------------------
# 1. PREPARAR DATOS
#---------------------------------------------
# Asegúrate de tener X (tus predictores) y Y (tu variable respuesta)
# X debe ser un data.frame solo con columnas numéricas
# Y debe ser un factor con clases como EE, PE, RP
# Asegurarse de que Y es factor
Y <- as.factor(data$`X1ª_eval`)  # o data$Mejor_resp si usas esa
# Eliminar columnas de respuesta del X
X <- data %>%
select(-c(`X1ª_eval`, `Mejor_resp`)) %>%
select(where(is.numeric)) %>%
as.data.frame()
# Convertir Y a numérico para XGBoost
Y_num <- as.numeric(Y) - 1
# Entrenar modelo con todas las variables para obtener importancia
dtrain_full <- xgb.DMatrix(data = as.matrix(X), label = Y_num)
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_num)),
eval_metric = "mlogloss",
max_depth = 3,
eta = 0.1,
verbosity = 0
)
set.seed(123)
xgb_model <- xgboost(
params = params,
data = dtrain_full,
nrounds = 50,
verbose = 0
)
#---------------------------------------------
# 2. SELECCIONAR TOP VARIABLES IMPORTANTES
#---------------------------------------------
importance <- xgb.importance(model = xgb_model)
top_vars <- importance$Feature[1:10]  # puedes usar 15 o 20 si quieres
# Crear nuevo X con solo esas variables
X_top <- X[, top_vars, drop = FALSE]
# Asegurar nombres para XGBoost y SHAP
X_matrix_named <- as.matrix(X_top)
colnames(X_matrix_named) <- colnames(X_top)
#---------------------------------------------
# 3. ENTRENAR MODELO FINAL PARA SHAP
#---------------------------------------------
library(xgboost)
library(SHAPforxgboost)
# 1. Entrenamiento con top variables
dtrain <- xgb.DMatrix(data = X_matrix_named, label = Y_num)
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_num)),
eval_metric = "mlogloss",
max_depth = 3,
eta = 0.1,
verbosity = 0
)
xgb_model_top <- xgboost(
params = params,
data = dtrain,
nrounds = 50,
verbose = 0
)
# 2. Calcular SHAP
shap_values <- shap.values(
xgb_model = xgb_model_top,
X_train = X_matrix_named
)
library(caret)
library(xgboost)
library(openxlsx)
library(xgboost)
data <- read.xlsx("datos_PCA.xlsx")
# Cargar librerías necesarias
library(xgboost)
library(SHAPforxgboost)
library(dplyr)
#---------------------------------------------
# 1. PREPARAR DATOS
#---------------------------------------------
# Asegúrate de tener X (tus predictores) y Y (tu variable respuesta)
# X debe ser un data.frame solo con columnas numéricas
# Y debe ser un factor con clases como EE, PE, RP
# Asegurarse de que Y es factor
Y <- as.factor(data$`X1ª_eval`)  # o data$Mejor_resp si usas esa
# Eliminar columnas de respuesta del X
X <- data %>%
select(-c(`X1ª_eval`, `Mejor_resp`)) %>%
select(where(is.numeric)) %>%
as.data.frame()
# Convertir Y a numérico para XGBoost
Y_num <- as.numeric(Y) - 1
# Entrenar modelo con todas las variables para obtener importancia
dtrain_full <- xgb.DMatrix(data = as.matrix(X), label = Y_num)
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_num)),
eval_metric = "mlogloss",
max_depth = 3,
eta = 0.1,
verbosity = 0
)
set.seed(123)
xgb_model <- xgboost(
params = params,
data = dtrain_full,
nrounds = 50,
verbose = 0
)
#---------------------------------------------
# 2. SELECCIONAR TOP VARIABLES IMPORTANTES
#---------------------------------------------
importance <- xgb.importance(model = xgb_model)
top_vars <- importance$Feature[1:10]  # puedes usar 15 o 20 si quieres
# Crear nuevo X con solo esas variables
X_top <- X[, top_vars, drop = FALSE]
# Asegurar nombres para XGBoost y SHAP
X_matrix_named <- as.matrix(X_top)
colnames(X_matrix_named) <- colnames(X_top)
#---------------------------------------------
# 3. ENTRENAR MODELO FINAL PARA SHAP
#---------------------------------------------
library(xgboost)
library(SHAPforxgboost)
# 1. Entrenamiento con top variables
dtrain <- xgb.DMatrix(data = X_matrix_named, label = Y_num)
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_num)),
eval_metric = "mlogloss",
max_depth = 3,
eta = 0.1,
verbosity = 0
)
xgb_model_top <- xgboost(
params = params,
data = dtrain,
nrounds = 50,
verbose = 0
)
# 2. Calcular SHAP
shap_values <- shap.values(
xgb_model = xgb_model_top,
X_train = X_matrix_named
)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(xgboost)
library(openxlsx)
library(xgboost)
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
library(caret)
library(xgboost)
library(openxlsx)
library(xgboost)
data <- read.xlsx("datos_PCA.xlsx")
# Y debe ser un factor con las clases (por ejemplo: EE, PE, RP)
Y <- as.factor(data$`X1ª_eval`)
# X debe ser un data.frame solo con variables numéricas
X <- data %>%
select(-c(`X1ª_eval`, `Mejor_resp`)) %>%  # quitar variable respuesta
select(where(is.numeric)) %>%
as.data.frame()
# Convertir a matriz y asegurarte de que los nombres sean válidos
X_matrix <- as.matrix(X)
colnames(X_matrix) <- make.names(colnames(X), unique = TRUE)
# Codificar las clases como enteros (XGBoost usa 0, 1, 2, ...)
Y_num <- as.numeric(Y) - 1
dtrain <- xgb.DMatrix(data = X_matrix, label = Y_num)
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_num)),
eval_metric = "mlogloss",
max_depth = 3,
eta = 0.1
)
set.seed(123)
xgb_model <- xgboost(
data = dtrain,
params = params,
nrounds = 50,
verbose = 0
)
# Calcular SHAP para todas las muestras
shap_values <- shap.values(xgb_model = xgb_model, X_train = X_matrix)
dtrain <- xgb.DMatrix(data = X_matrix, label = Y_num)
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_num)),
eval_metric = "mlogloss",
max_depth = 3,
eta = 0.1
)
xgb_model <- xgboost(
params = params,
data = dtrain,
nrounds = 50,
verbose = 0
)
# Calcular SHAP
shap_values <- shap.values(xgb_model = xgb_model, X_train = X_matrix)
# Calcular SHAP
shap_values <- shap.values(xgb_model = xgb_model, X_train = X_matrix)
# Calcular SHAP
shap_values <- shap.values(xgb_model = xgb_model, X_train = X_matrix)
# Calcular SHAP
shap_values <- shap.values(xgb_model = xgb_model, X_train = X_matrix)
# Calcular SHAP
shap_values <- shap.values(xgb_model = xgb_model, X_train = X_matrix)
# Calcular SHAP
shap_values <- shap.values(xgb_model = xgb_model, X_train = X_matrix)
# Calcular SHAP
shap_values <- shap.values(xgb_model = xgb_model, X_train = X_matrix)
# Calcular SHAP
shap_values <- shap.values(xgb_model = xgb_model, X_train = X_matrix)
# Calcular SHAP
shap_values <- shap.values(xgb_model = xgb_model, X_train = X_matrix)
# Calcular SHAP
shap_values <- shap.values(xgb_model = xgb_model, X_train = X_matrix)
# Calcular SHAP
shap_values <- shap.values(xgb_model = xgb_model, X_train = X_matrix)
library(caret)
# Entrena XGBoost multiclase
modelo_xgb <- train(
x = X_matrix,
y = Y,
method = "xgbTree",
trControl = trainControl(method = "cv", number = 5),
tuneLength = 3
)
# Obtener importancia de variables
importancia <- varImp(modelo_xgb)
plot(importancia, top = 20)
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(xgboost)
library(openxlsx)
library(xgboost)
data <- read.xlsx("datos_PCA.xlsx")
# Y debe ser un factor con las clases (por ejemplo: EE, PE, RP)
Y <- as.factor(data$`X1ª_eval`)
# X debe ser un data.frame solo con variables numéricas
X <- data %>%
select(-c(`X1ª_eval`, `Mejor_resp`)) %>%  # quitar variable respuesta
select(where(is.numeric)) %>%
as.data.frame()
# Convertir a matriz y asegurarte de que los nombres sean válidos
X_matrix <- as.matrix(X)
colnames(X_matrix) <- make.names(colnames(X), unique = TRUE)
# Codificar las clases como enteros (XGBoost usa 0, 1, 2, ...)
Y_num <- as.numeric(Y) - 1
dtrain <- xgb.DMatrix(data = X_matrix, label = Y_num)
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_num)),
eval_metric = "mlogloss",
max_depth = 3,
eta = 0.1
)
xgb_model <- xgboost(
params = params,
data = dtrain,
nrounds = 50,
verbose = 0
)
library(caret)
# Entrena XGBoost multiclase
modelo_xgb <- train(
x = X_matrix,
y = Y,
method = "xgbTree",
trControl = trainControl(method = "cv", number = 5),
tuneLength = 3
)
# Obtener importancia de variables
importancia <- varImp(modelo_xgb)
plot(importancia, top = 20)
dtrain <- xgb.DMatrix(data = X_matrix, label = Y_num)
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_num)),
eval_metric = "mlogloss",
max_depth = 3,
eta = 0.1
)
xgb_model <- xgboost(
params = params,
data = dtrain,
nrounds = 50,
verbose = 0
)
# Predecir probabilidades para cada clase
pred_probs <- predict(xgb_model, newdata = dtrain)
# Convertir a matriz: 1 fila por observación, 1 columna por clase
pred_matrix <- matrix(pred_probs, ncol = length(unique(Y_num)), byrow = TRUE)
# Elegir clase con mayor probabilidad
pred_class <- max.col(pred_matrix) - 1  # clases comienzan en 0
# Comparar con la real
library(caret)
pred_factor <- factor(pred_class, levels = 0:(length(levels(Y)) - 1), labels = levels(Y))
confusionMatrix(pred_factor, Y)
library(caret)
library(xgboost)
library(openxlsx)
library(xgboost)
data <- read.xlsx("datos_PCA.xlsx")
# Y debe ser un factor con las clases (por ejemplo: EE, PE, RP)
Y <- as.factor(data$`X1ª_eval`)
# X debe ser un data.frame solo con variables numéricas
X <- data %>%
select(-c(`X1ª_eval`, `Mejor_resp`)) %>%  # quitar variable respuesta
select(where(is.numeric)) %>%
as.data.frame()
# Convertir a matriz y asegurarte de que los nombres sean válidos
X_matrix <- as.matrix(X)
colnames(X_matrix) <- make.names(colnames(X), unique = TRUE)
library(xgboost)
library(caret)
# Matriz y etiquetas (deben estar definidos ya)
# X_matrix: matriz numérica con nombres válidos
# Y: factor con etiquetas
Y_num <- as.numeric(Y) - 1
n <- nrow(X_matrix)
preds <- numeric(n)
# Loop Leave-One-Out
for (i in 1:n) {
train_idx <- setdiff(1:n, i)
dtrain <- xgb.DMatrix(data = X_matrix[train_idx, ], label = Y_num[train_idx])
dtest  <- xgb.DMatrix(data = X_matrix[i, , drop = FALSE])
model <- xgboost(
params = list(
objective = "multi:softprob",
num_class = length(unique(Y_num)),
eval_metric = "mlogloss",
max_depth = 3,
eta = 0.1,
verbosity = 0
),
data = dtrain,
nrounds = 50,
verbose = 0
)
prob <- predict(model, newdata = dtest)
pred_class <- which.max(prob) - 1
preds[i] <- pred_class
}
# Evaluar rendimiento
pred_factor <- factor(preds, levels = 0:(length(levels(Y)) - 1), labels = levels(Y))
confusionMatrix(pred_factor, Y)
# Entrena un modelo completo (sin LOO) solo para obtener importancia
dtrain_full <- xgb.DMatrix(data = X_matrix, label = Y_num)
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_num)),
eval_metric = "mlogloss",
max_depth = 3,
eta = 0.1
)
xgb_model_full <- xgboost(
params = params,
data = dtrain_full,
nrounds = 50,
verbose = 0
)
# Extraer importancia de variables
importance <- xgb.importance(model = xgb_model_full)
# Mostrar top 10
print(importance[1:10, ])
# Escoge top 10 variables más importantes
top_vars <- importance$Feature[1:10]  # o 15 o 20 si prefieres
# Crear nuevo X reducido
X_top <- X[, top_vars, drop = FALSE]
X_top_matrix <- as.matrix(X_top)
n <- nrow(X_top_matrix)
preds <- numeric(n)
for (i in 1:n) {
train_idx <- setdiff(1:n, i)
dtrain <- xgb.DMatrix(data = X_top_matrix[train_idx, ], label = Y_num[train_idx])
dtest  <- xgb.DMatrix(data = X_top_matrix[i, , drop = FALSE])
model <- xgboost(
params = params,
data = dtrain,
nrounds = 50,
verbose = 0
)
prob <- predict(model, newdata = dtest)
pred_class <- which.max(prob) - 1
preds[i] <- pred_class
}
# Evaluar
pred_factor <- factor(preds, levels = 0:(length(levels(Y)) - 1), labels = levels(Y))
confusionMatrix(pred_factor, Y)
install.packages("ordinalNet")
knitr::opts_chunk$set(echo = TRUE)
install.packages("ordinalNet")
library(openxlsx)
library(ordinalNet)
library(openxlsx)
library(ordinalNet)
data <- read.xlsx("datos_PCA.xlsx")
knitr::opts_chunk$set(echo = TRUE)
library(openxlsx)
library(ordinalNet)
X <- read.xlsx("datos_PCA.xlsx")
knitr::opts_chunk$set(echo = TRUE)
library(openxlsx)
library(ordinalNet)
X <- read.xlsx("datos_PCA.xlsx")
# Asegúrate de que la variable respuesta es factor ORDENADO
Y_ord <- factor(Y, levels = c("RP", "EE", "PE"), ordered = TRUE)  # Ajusta el orden si hace falta
# Cargar paquetes necesarios
library(openxlsx)
library(ordinalNet)
library(caret)
# Leer los datos desde Excel
data <- read.xlsx("datos_PCA.xlsx")
# Verifica las primeras columnas
str(data)
# ⚠️ Ajustar nombre de tu variable respuesta y convertirla en factor ORDENADO
# Asegúrate de que esta variable existe en tu Excel
Y <- factor(data$`X1ª_eval`, levels = c("RP", "PE", "EE"), ordered = TRUE)
# Filtrar solo variables numéricas para X (y quitar columnas respuesta si están)
X <- data[, sapply(data, is.numeric)]
X_matrix <- as.matrix(X)
# Ajustar el modelo con penalización Elastic Net
modelo_ord <- ordinalNet(
x = X_matrix,
y = Y,
family = "cumulative",
link = "logit",
alpha = 0.5,          # 0 = Ridge, 1 = Lasso, 0.5 = Elastic Net
standardize = TRUE
)
# Mostrar coeficientes (puedes exportar si quieres)
print(coef(modelo_ord))
# Predecir clases sobre los mismos datos
pred <- predict(modelo_ord, newx = X_matrix, type = "class")
# Evaluar predicciones
conf_matrix <- confusionMatrix(
data = factor(pred, levels = levels(Y)),
reference = Y
)
print(conf_matrix)
# Cargar librerías necesarias
library(openxlsx)
library(ordinalNet)
library(caret)
library(xgboost)
# 1. Leer los datos desde el archivo Excel
data <- read.xlsx("datos_PCA.xlsx")
# 2. Preparar la variable respuesta como factor ORDENADO
Y_ord <- factor(data$`X1ª_eval`, levels = c("RP", "PE", "EE"), ordered = TRUE)
# 3. Preparar los predictores numéricos
X <- data[, sapply(data, is.numeric)]
X_matrix <- as.matrix(X)
# 4. Entrenar un modelo XGBoost para obtener importancia de variables
Y_num <- as.numeric(Y_ord) - 1
dtrain_full <- xgb.DMatrix(data = X_matrix, label = Y_num)
params <- list(
objective = "multi:softprob",
num_class = length(unique(Y_num)),
eval_metric = "mlogloss",
max_depth = 3,
eta = 0.1
)
xgb_model_full <- xgboost(
params = params,
data = dtrain_full,
nrounds = 50,
verbose = 0
)
# 5. Seleccionar las 10 variables más importantes
importance <- xgb.importance(model = xgb_model_full)
top_vars <- importance$Feature[1:10]
# 6. Crear nuevo conjunto de predictores con solo las variables importantes
X_top <- data[, top_vars]
X_matrix_top <- as.matrix(X_top)
# 7. Entrenar modelo ordinal con validación cruzada Leave-One-Out
cv_model <- ordinalNetCV(
x = X_matrix_top,
y = Y_ord,
family = "cumulative",
link = "logit",
alpha = 0.5,
nFolds = nrow(X_matrix_top)  # LOO
)
# 8. Predecir con el modelo óptimo (lambda mínimo)
pred_cv <- predict(cv_model$fit, newx = X_matrix_top, type = "class")
# 9. Evaluar rendimiento con matriz de confusión
confusion <- confusionMatrix(factor(pred_cv, levels = levels(Y_ord)), Y_ord)
print(confusion)
# 10. Ver coeficientes del modelo final (opcional)
print(coef(cv_model$fit))
