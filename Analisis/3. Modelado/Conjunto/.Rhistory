data <- read.xlsx("datos_PCA.xlsx")
# Preparaci贸n de datos
X <- data[, setdiff(names(data), c("X1陋_eval", "Mejor_resp"))]
library(openxlsx)
library(dplyr)
library(fastDummies)
library(ropls)
library(ggplot2)
library(ggrepel)
library(caret)
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
# Preparaci贸n de datos
X <- data[, setdiff(names(data), c("X1陋_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1陋_eval`)  # aseg煤rate del nombre exacto si tiene tildes o espacios
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
# Entrenamiento sin la i-茅sima observaci贸n
X_train <- X[-i, ]
Y_train <- Y[-i]
# Entrenamiento del modelo RF
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predicci贸n de la i-茅sima observaci贸n
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardamos predicci贸n
mypred[i] <- as.character(pred_i)
}
library(openxlsx)
library(dplyr)
library(fastDummies)
library(ropls)
library(ggplot2)
library(ggrepel)
library(caret)
# Cargar la librer铆a necesaria
library(randomForest)
data <- read.xlsx("datos_PCA.xlsx")
library(openxlsx)
library(dplyr)
library(fastDummies)
library(ropls)
library(ggplot2)
library(ggrepel)
library(caret)
library(randomForest)
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
# Preparaci贸n de datos
X <- data[, setdiff(names(data), c("X1陋_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1陋_eval`)  # aseg煤rate del nombre exacto si tiene tildes o espacios
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
# Entrenamiento sin la i-茅sima observaci贸n
X_train <- X[-i, ]
Y_train <- Y[-i]
# Entrenamiento del modelo RF
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predicci贸n de la i-茅sima observaci贸n
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardamos predicci贸n
mypred[i] <- as.character(pred_i)
}
# Convertimos a factor con los mismos niveles que Y real
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusi贸n
confusionMatrix(mypred, Y)
# Entrenamos un modelo RF completo para an谩lisis SHAP
rf_full <- randomForest(x = X, y = Y, ntree = 500, importance = TRUE)
# Para importancia est谩ndar (Gini o permutaci贸n)
importance(rf_full)
# Para SHAP con iml
library(iml)
install.packages('iml')
# Entrenamos un modelo RF completo para an谩lisis SHAP
rf_full <- randomForest(x = X, y = Y, ntree = 500, importance = TRUE)
# Para importancia est谩ndar (Gini o permutaci贸n)
importance(rf_full)
# Para SHAP con iml
library(iml)
predictor_rf <- Predictor$new(rf_full, data = X, y = Y, type = "prob")  # o type = "response" si no probabil铆stico
shap <- Shapley$new(predictor_rf, x.interest = X[1, ])  # Puedes iterar sobre varias filas
# Ver SHAP plot
plot(shap)
# Si tienes un data.frame con importancia SHAP
shap_importance <- data.frame(Feature = names_shap, Value = shap_values)  # cambia esto por tu objeto real
predictor_rf <- Predictor$new(rf_full, data = X, y = Y, type = "prob")
shap <- FeatureImp$new(predictor_rf, loss = "ce")
shap_df <- shap$results
print(shap_df)
shap_importance <- data.frame(Feature = names_shap, Value = shap_values)  # cambia esto por tu objeto real
# 1. Entrenar el modelo completo con todos los datos
rf_full <- randomForest(x = X, y = Y, ntree = 500, importance = TRUE)
# 2. Crear objeto Predictor para iml
predictor_rf <- Predictor$new(rf_full, data = X, y = Y, type = "prob")
# 3. Calcular importancia SHAP aproximada (FeatureImp)
shap <- FeatureImp$new(predictor_rf, loss = "ce")  # ce = cross-entropy para clasificaci贸n
# 4. Extraer las top 10 variables m谩s importantes
shap_df <- shap$results
top10 <- shap_df$feature[order(shap_df$importance, decreasing = TRUE)][1:10]
print(top10)
# 5. Reducir los datos a esas variables
X_top <- X[, top10]
# 6. Reentrenar modelo con solo esas variables
rf_top <- randomForest(x = X_top, y = Y, ntree = 500)
# 7. Evaluar el modelo
pred_top <- predict(rf_top, newdata = X_top)
conf_matrix <- confusionMatrix(pred_top, Y)
print(conf_matrix)
# 1. Entrenar el modelo completo con todos los datos
rf_full <- randomForest(x = X, y = Y, ntree = 500, importance = TRUE)
# 2. Crear objeto Predictor para iml
predictor_rf <- Predictor$new(rf_full, data = X, y = Y, type = "prob")
# 3. Calcular importancia SHAP aproximada (FeatureImp)
shap <- FeatureImp$new(predictor_rf, loss = "ce")  # ce = cross-entropy para clasificaci贸n
# 4. Extraer las top 10 variables m谩s importantes
shap_df <- shap$results
top10 <- shap_df$feature[order(shap_df$importance, decreasing = TRUE)][1:10]
print(top10)
# 5. Reducir los datos a esas variables
X_top <- X[, top10]
# 6. Reentrenar modelo con solo esas variables
rf_top <- randomForest(x = X_top, y = Y, ntree = 500)
# 1. Vector para almacenar predicciones
mypred <- character(nrow(X_top))
names(mypred) <- rownames(X_top)
# 2. Bucle LOO usando solo las top 10 variables
for (i in 1:nrow(X_top)) {
# Datos de entrenamiento sin la observaci贸n i
X_train <- X_top[-i, ]
Y_train <- Y[-i]
# Entrenar modelo RF con esas observaciones
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predecir la observaci贸n dejada fuera
X_test <- X_top[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardar la predicci贸n
mypred[i] <- as.character(pred_i)
}
# 3. Convertir a factor con niveles correctos
mypred <- factor(mypred, levels = levels(Y))
# 4. Matriz de confusi贸n LOO
confusionMatrix(mypred, Y)
table(Y)
# 1. Entrenar el modelo completo con todos los datos
rf_full <- randomForest(x = X, y = Y, ntree = 500, importance = TRUE)
# 2. Crear objeto Predictor para iml
predictor_rf <- Predictor$new(rf_full, data = X, y = Y, type = "prob")
# 3. Calcular importancia SHAP aproximada (FeatureImp)
shap <- FeatureImp$new(predictor_rf, loss = "ce")  # ce = cross-entropy para clasificaci贸n
# 4. Extraer las top 10 variables m谩s importantes
shap_df <- shap$results
top10 <- shap_df$feature[order(shap_df$importance, decreasing = TRUE)][1:10]
print(top10)
# 5. Reducir los datos a esas variables
X_top <- X[, top10]
# 6. Reentrenar modelo con solo esas variables
rf_top <- randomForest(x = X_top, y = Y, ntree = 500)
# 1. Vector para almacenar predicciones
mypred <- character(nrow(X_top))
names(mypred) <- rownames(X_top)
for (i in 1:nrow(X_top)) {
X_train <- X_top[-i, ]
Y_train <- Y[-i]
#  Aseg煤rate de que Y_train conserve todos los niveles
Y_train <- factor(Y_train, levels = levels(Y))
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
X_test <- X_top[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
mypred[i] <- as.character(pred_i)
}
mypred <- factor(mypred, levels = levels(Y))
confusionMatrix(mypred, Y)
# 3. Convertir a factor con niveles correctos
mypred <- factor(mypred, levels = levels(Y))
# 4. Matriz de confusi贸n LOO
confusionMatrix(mypred, Y)
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
# 1. Preparar datos
Y <- as.factor(data$`X1陋_eval`)
X <- data %>%
select(-c('X1陋_eval', 'Mejor_resp')) %>%
select(where(is.numeric)) %>%
as.data.frame()
# Preparaci贸n de datos
X <- data[, setdiff(names(data), c("X1陋_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1陋_eval`)  # aseg煤rate del nombre exacto si tiene tildes o espacios
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
# Entrenamiento sin la i-茅sima observaci贸n
X_train <- X[-i, ]
Y_train <- Y[-i]
# Entrenamiento del modelo RF
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predicci贸n de la i-茅sima observaci贸n
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardamos predicci贸n
mypred[i] <- as.character(pred_i)
}
# Convertimos a factor con los mismos niveles que Y real
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusi贸n
confusionMatrix(mypred, Y)
data <- read.xlsx("datos_PCA.xlsx")
# Preparaci贸n de datos
X <- data[, setdiff(names(data), c("X1陋_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1陋_eval`)  # aseg煤rate del nombre exacto si tiene tildes o espacios
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
# Entrenamiento sin la i-茅sima observaci贸n
X_train <- X[-i, ]
Y_train <- Y[-i]
# Entrenamiento del modelo RF
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predicci贸n de la i-茅sima observaci贸n
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardamos predicci贸n
mypred[i] <- as.character(pred_i)
}
# Convertimos a factor con los mismos niveles que Y real
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusi贸n
confusionMatrix(mypred, Y)
library(openxlsx)
library(dplyr)
library(fastDummies)
library(ropls)
library(ggplot2)
library(ggrepel)
library(caret)
library(randomForest)
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
# Preparaci贸n de datos
X <- data[, setdiff(names(data), c("X1陋_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1陋_eval`)  # aseg煤rate del nombre exacto si tiene tildes o espacios
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
# Entrenamiento sin la i-茅sima observaci贸n
X_train <- X[-i, ]
Y_train <- Y[-i]
# Entrenamiento del modelo RF
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predicci贸n de la i-茅sima observaci贸n
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardamos predicci贸n
mypred[i] <- as.character(pred_i)
}
# Convertimos a factor con los mismos niveles que Y real
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusi贸n
confusionMatrix(mypred, Y)
library(openxlsx)
library(dplyr)
library(fastDummies)
library(ropls)
library(ggplot2)
library(ggrepel)
library(caret)
library(randomForest)
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
# Preparaci贸n de datos
X <- data[, setdiff(names(data), c("X1陋_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1陋_eval`)  # aseg煤rate del nombre exacto si tiene tildes o espacios
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
# Entrenamiento sin la i-茅sima observaci贸n
X_train <- X[-i, ]
Y_train <- Y[-i]
# Entrenamiento del modelo RF
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predicci贸n de la i-茅sima observaci贸n
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardamos predicci贸n
mypred[i] <- as.character(pred_i)
}
# Convertimos a factor con los mismos niveles que Y real
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusi贸n
confusionMatrix(mypred, Y)
# Entrenamos un modelo RF completo para an谩lisis SHAP
rf_full <- randomForest(x = X, y = Y, ntree = 500, importance = TRUE)
# Para importancia est谩ndar (Gini o permutaci贸n)
importance(rf_full)
# Para SHAP con iml
library(iml)
predictor_rf <- Predictor$new(rf_full, data = X, y = Y, type = "prob")  # o type = "response" si no probabil铆stico
shap <- Shapley$new(predictor_rf, x.interest = X[1, ])  # Puedes iterar sobre varias filas
# Ver SHAP plot
plot(shap)
data <- read.xlsx("datos_PCA.xlsx")
# Preparaci贸n de datos
X <- data[, setdiff(names(data), c("X1陋_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1陋_eval`)
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
X_train <- X[-i, ]
#  Esto asegura que RF siempre conozca las 3 clases
Y_train <- factor(Y[-i], levels = levels(Y))
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
mypred[i] <- as.character(pred_i)
}
# Convertimos predicciones a factor con los mismos niveles
mypred <- factor(mypred, levels = levels(Y))
# Evaluamos
confusionMatrix(mypred, Y)
data <- read.xlsx("datos_PCA.xlsx")
# Suponemos que tienes cargado el paquete randomForest y caret
# X = variables num茅ricas predictoras
X <- data[, setdiff(names(data), c("X1陋_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$X1陋_eval)
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
library(randomForest)
# LOO
for (i in 1:nrow(X)) {
X_train <- X[-i, ]
Y_train <- Y[-i]
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500, mtry = floor(sqrt(ncol(X_train))))
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
mypred[i] <- as.character(pred_i)
}
# Convertir a factor con niveles iguales a Y
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusi贸n
library(caret)
confusionMatrix(mypred, Y)
knitr::opts_chunk$set(echo = TRUE)
# Instala si no los tienes
#install.packages(c("readxl", "randomForest", "caret", "fastshap", "dplyr"))
# Cargar librer铆as
library(openxlsx)
library(randomForest)
library(caret)
library(fastshap)
library(dplyr)
# Leer Excel
data <- read.xlsx("datos_PCA.xlsx")
# 1. Preparar X e Y
Y <- as.factor(data$X1陋_eval)
X <- data %>%
select(-c('X1陋_eval', 'Mejor_resp')) %>%
select(where(is.numeric)) %>%
as.data.frame()
# Suponemos que tienes cargado el paquete randomForest y caret
# X = variables num茅ricas predictoras
data$ECOG <- as.numeric(data$ECOG)
X <- data[, setdiff(names(data), c("X1陋_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$X1陋_eval)
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
library(randomForest)
# LOO
for (i in 1:nrow(X)) {
X_train <- X[-i, ]
Y_train <- Y[-i]
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500, mtry = floor(sqrt(ncol(X_train))))
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
mypred[i] <- as.character(pred_i)
}
# Convertir a factor con niveles iguales a Y
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusi贸n
library(caret)
confusionMatrix(mypred, Y)
# Suponemos que tienes cargado el paquete randomForest y caret
# X = variables num茅ricas predictoras
data$ECOG <- as.numeric(data$ECOG)
X <- data[, setdiff(names(data), c("X1陋_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$X1陋_eval)
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
library(randomForest)
# LOO
for (i in 1:nrow(X)) {
X_train <- X[-i, ]
Y_train <- Y[-i]
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500, mtry = floor(sqrt(ncol(X_train))))
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
mypred[i] <- as.character(pred_i)
}
# Convertir a factor con niveles iguales a Y
mypred <- factor(mypred, levels = levels(Y))
confusionMatrix(mypred, Y)
knitr::opts_chunk$set(echo = TRUE)
# Instala si no los tienes
#install.packages(c("readxl", "randomForest", "caret", "fastshap", "dplyr"))
# Cargar librer铆as
library(openxlsx)
library(randomForest)
library(caret)
library(fastshap)
library(dplyr)
# Leer Excel
data <- read.xlsx("datos_PCA.xlsx")
# 1. Preparar X e Y
Y <- as.factor(data$X1陋_eval)
X <- data %>%
select(-c('X1陋_eval', 'Mejor_resp')) %>%
select(where(is.numeric)) %>%
as.data.frame()
library(dplyr)
library(caret)
library(randomForest)
# 1. Preparar X e Y
Y <- as.factor(data$X1陋_eval)
X <- data %>%
select(-c('X1陋_eval', 'Mejor_resp')) %>%
select(where(is.numeric)) %>%
as.data.frame()  # convertir a data.frame por si train lo necesita
library(openxlsx)
library(dplyr)
library(fastDummies)
library(ropls)
library(ggplot2)
library(ggrepel)
library(caret)
library(randomForest)
data <- read.xlsx("datos_PCA.xlsx")
data$ECOG <- as.numeric(data$ECOG)
X <- data[, setdiff(names(data), c("X1陋_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$X1陋_eval)
# Creamos un vector vac铆o para almacenar las predicciones
mypred <- character(nrow(X))  # o factor() si prefieres
names(mypred) <- rownames(X)
# LOO
for (i in 1:nrow(X)) {
# Creamos datos de entrenamiento sin la i-茅sima fila
X_train <- X[-i, ]
Y_train <- Y[-i]
# Modelo PLS-DA con entrenamiento
pls_model <- opls(X_train, Y_train, predI = 2, orthoI = 0)
# Predicci贸n de la muestra dejada fuera
X_test <- X[i, , drop = FALSE]
pred_i <- predict(pls_model, newdata = X_test)
# Guardamos la predicci贸n
mypred[i] <- as.character(pred_i)
}
# Convertimos a factor con los mismos niveles que Y
mypred <- factor(mypred, levels = levels(Y))
# Confusion matrix
confusionMatrix(mypred, Y)
