data <- read.xlsx("datos_PCA.xlsx")
# Preparación de datos
X <- data[, setdiff(names(data), c("X1ª_eval", "Mejor_resp"))]
library(openxlsx)
library(dplyr)
library(fastDummies)
library(ropls)
library(ggplot2)
library(ggrepel)
library(caret)
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
# Preparación de datos
X <- data[, setdiff(names(data), c("X1ª_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1ª_eval`)  # asegúrate del nombre exacto si tiene tildes o espacios
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
# Entrenamiento sin la i-ésima observación
X_train <- X[-i, ]
Y_train <- Y[-i]
# Entrenamiento del modelo RF
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predicción de la i-ésima observación
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardamos predicción
mypred[i] <- as.character(pred_i)
}
library(openxlsx)
library(dplyr)
library(fastDummies)
library(ropls)
library(ggplot2)
library(ggrepel)
library(caret)
# Cargar la librería necesaria
library(randomForest)
data <- read.xlsx("datos_PCA.xlsx")
library(openxlsx)
library(dplyr)
library(fastDummies)
library(ropls)
library(ggplot2)
library(ggrepel)
library(caret)
library(randomForest)
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
# Preparación de datos
X <- data[, setdiff(names(data), c("X1ª_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1ª_eval`)  # asegúrate del nombre exacto si tiene tildes o espacios
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
# Entrenamiento sin la i-ésima observación
X_train <- X[-i, ]
Y_train <- Y[-i]
# Entrenamiento del modelo RF
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predicción de la i-ésima observación
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardamos predicción
mypred[i] <- as.character(pred_i)
}
# Convertimos a factor con los mismos niveles que Y real
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusión
confusionMatrix(mypred, Y)
# Entrenamos un modelo RF completo para análisis SHAP
rf_full <- randomForest(x = X, y = Y, ntree = 500, importance = TRUE)
# Para importancia estándar (Gini o permutación)
importance(rf_full)
# Para SHAP con iml
library(iml)
install.packages('iml')
# Entrenamos un modelo RF completo para análisis SHAP
rf_full <- randomForest(x = X, y = Y, ntree = 500, importance = TRUE)
# Para importancia estándar (Gini o permutación)
importance(rf_full)
# Para SHAP con iml
library(iml)
predictor_rf <- Predictor$new(rf_full, data = X, y = Y, type = "prob")  # o type = "response" si no probabilístico
shap <- Shapley$new(predictor_rf, x.interest = X[1, ])  # Puedes iterar sobre varias filas
# Ver SHAP plot
plot(shap)
# Si tienes un data.frame con importancia SHAP
shap_importance <- data.frame(Feature = names_shap, Value = shap_values)  # cambia esto por tu objeto real
predictor_rf <- Predictor$new(rf_full, data = X, y = Y, type = "prob")
shap <- FeatureImp$new(predictor_rf, loss = "ce")
shap_df <- shap$results
print(shap_df)
shap_importance <- data.frame(Feature = names_shap, Value = shap_values)  # cambia esto por tu objeto real
# 1. Entrenar el modelo completo con todos los datos
rf_full <- randomForest(x = X, y = Y, ntree = 500, importance = TRUE)
# 2. Crear objeto Predictor para iml
predictor_rf <- Predictor$new(rf_full, data = X, y = Y, type = "prob")
# 3. Calcular importancia SHAP aproximada (FeatureImp)
shap <- FeatureImp$new(predictor_rf, loss = "ce")  # ce = cross-entropy para clasificación
# 4. Extraer las top 10 variables más importantes
shap_df <- shap$results
top10 <- shap_df$feature[order(shap_df$importance, decreasing = TRUE)][1:10]
print(top10)
# 5. Reducir los datos a esas variables
X_top <- X[, top10]
# 6. Reentrenar modelo con solo esas variables
rf_top <- randomForest(x = X_top, y = Y, ntree = 500)
# 7. Evaluar el modelo
pred_top <- predict(rf_top, newdata = X_top)
conf_matrix <- confusionMatrix(pred_top, Y)
print(conf_matrix)
# 1. Entrenar el modelo completo con todos los datos
rf_full <- randomForest(x = X, y = Y, ntree = 500, importance = TRUE)
# 2. Crear objeto Predictor para iml
predictor_rf <- Predictor$new(rf_full, data = X, y = Y, type = "prob")
# 3. Calcular importancia SHAP aproximada (FeatureImp)
shap <- FeatureImp$new(predictor_rf, loss = "ce")  # ce = cross-entropy para clasificación
# 4. Extraer las top 10 variables más importantes
shap_df <- shap$results
top10 <- shap_df$feature[order(shap_df$importance, decreasing = TRUE)][1:10]
print(top10)
# 5. Reducir los datos a esas variables
X_top <- X[, top10]
# 6. Reentrenar modelo con solo esas variables
rf_top <- randomForest(x = X_top, y = Y, ntree = 500)
# 1. Vector para almacenar predicciones
mypred <- character(nrow(X_top))
names(mypred) <- rownames(X_top)
# 2. Bucle LOO usando solo las top 10 variables
for (i in 1:nrow(X_top)) {
# Datos de entrenamiento sin la observación i
X_train <- X_top[-i, ]
Y_train <- Y[-i]
# Entrenar modelo RF con esas observaciones
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predecir la observación dejada fuera
X_test <- X_top[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardar la predicción
mypred[i] <- as.character(pred_i)
}
# 3. Convertir a factor con niveles correctos
mypred <- factor(mypred, levels = levels(Y))
# 4. Matriz de confusión LOO
confusionMatrix(mypred, Y)
table(Y)
# 1. Entrenar el modelo completo con todos los datos
rf_full <- randomForest(x = X, y = Y, ntree = 500, importance = TRUE)
# 2. Crear objeto Predictor para iml
predictor_rf <- Predictor$new(rf_full, data = X, y = Y, type = "prob")
# 3. Calcular importancia SHAP aproximada (FeatureImp)
shap <- FeatureImp$new(predictor_rf, loss = "ce")  # ce = cross-entropy para clasificación
# 4. Extraer las top 10 variables más importantes
shap_df <- shap$results
top10 <- shap_df$feature[order(shap_df$importance, decreasing = TRUE)][1:10]
print(top10)
# 5. Reducir los datos a esas variables
X_top <- X[, top10]
# 6. Reentrenar modelo con solo esas variables
rf_top <- randomForest(x = X_top, y = Y, ntree = 500)
# 1. Vector para almacenar predicciones
mypred <- character(nrow(X_top))
names(mypred) <- rownames(X_top)
for (i in 1:nrow(X_top)) {
X_train <- X_top[-i, ]
Y_train <- Y[-i]
# 🔧 Asegúrate de que Y_train conserve todos los niveles
Y_train <- factor(Y_train, levels = levels(Y))
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
X_test <- X_top[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
mypred[i] <- as.character(pred_i)
}
mypred <- factor(mypred, levels = levels(Y))
confusionMatrix(mypred, Y)
# 3. Convertir a factor con niveles correctos
mypred <- factor(mypred, levels = levels(Y))
# 4. Matriz de confusión LOO
confusionMatrix(mypred, Y)
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
# 1. Preparar datos
Y <- as.factor(data$`X1ª_eval`)
X <- data %>%
select(-c('X1ª_eval', 'Mejor_resp')) %>%
select(where(is.numeric)) %>%
as.data.frame()
# Preparación de datos
X <- data[, setdiff(names(data), c("X1ª_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1ª_eval`)  # asegúrate del nombre exacto si tiene tildes o espacios
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
# Entrenamiento sin la i-ésima observación
X_train <- X[-i, ]
Y_train <- Y[-i]
# Entrenamiento del modelo RF
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predicción de la i-ésima observación
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardamos predicción
mypred[i] <- as.character(pred_i)
}
# Convertimos a factor con los mismos niveles que Y real
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusión
confusionMatrix(mypred, Y)
data <- read.xlsx("datos_PCA.xlsx")
# Preparación de datos
X <- data[, setdiff(names(data), c("X1ª_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1ª_eval`)  # asegúrate del nombre exacto si tiene tildes o espacios
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
# Entrenamiento sin la i-ésima observación
X_train <- X[-i, ]
Y_train <- Y[-i]
# Entrenamiento del modelo RF
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predicción de la i-ésima observación
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardamos predicción
mypred[i] <- as.character(pred_i)
}
# Convertimos a factor con los mismos niveles que Y real
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusión
confusionMatrix(mypred, Y)
library(openxlsx)
library(dplyr)
library(fastDummies)
library(ropls)
library(ggplot2)
library(ggrepel)
library(caret)
library(randomForest)
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
# Preparación de datos
X <- data[, setdiff(names(data), c("X1ª_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1ª_eval`)  # asegúrate del nombre exacto si tiene tildes o espacios
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
# Entrenamiento sin la i-ésima observación
X_train <- X[-i, ]
Y_train <- Y[-i]
# Entrenamiento del modelo RF
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predicción de la i-ésima observación
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardamos predicción
mypred[i] <- as.character(pred_i)
}
# Convertimos a factor con los mismos niveles que Y real
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusión
confusionMatrix(mypred, Y)
library(openxlsx)
library(dplyr)
library(fastDummies)
library(ropls)
library(ggplot2)
library(ggrepel)
library(caret)
library(randomForest)
data <- read.xlsx("datos_PCA.xlsx")
data <- read.xlsx("datos_PCA.xlsx")
# Preparación de datos
X <- data[, setdiff(names(data), c("X1ª_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1ª_eval`)  # asegúrate del nombre exacto si tiene tildes o espacios
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
# Entrenamiento sin la i-ésima observación
X_train <- X[-i, ]
Y_train <- Y[-i]
# Entrenamiento del modelo RF
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
# Predicción de la i-ésima observación
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
# Guardamos predicción
mypred[i] <- as.character(pred_i)
}
# Convertimos a factor con los mismos niveles que Y real
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusión
confusionMatrix(mypred, Y)
# Entrenamos un modelo RF completo para análisis SHAP
rf_full <- randomForest(x = X, y = Y, ntree = 500, importance = TRUE)
# Para importancia estándar (Gini o permutación)
importance(rf_full)
# Para SHAP con iml
library(iml)
predictor_rf <- Predictor$new(rf_full, data = X, y = Y, type = "prob")  # o type = "response" si no probabilístico
shap <- Shapley$new(predictor_rf, x.interest = X[1, ])  # Puedes iterar sobre varias filas
# Ver SHAP plot
plot(shap)
data <- read.xlsx("datos_PCA.xlsx")
# Preparación de datos
X <- data[, setdiff(names(data), c("X1ª_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$`X1ª_eval`)
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
# LOO con Random Forest
for (i in 1:nrow(X)) {
X_train <- X[-i, ]
# 🔧 Esto asegura que RF siempre conozca las 3 clases
Y_train <- factor(Y[-i], levels = levels(Y))
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500)
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
mypred[i] <- as.character(pred_i)
}
# Convertimos predicciones a factor con los mismos niveles
mypred <- factor(mypred, levels = levels(Y))
# Evaluamos
confusionMatrix(mypred, Y)
data <- read.xlsx("datos_PCA.xlsx")
# Suponemos que tienes cargado el paquete randomForest y caret
# X = variables numéricas predictoras
X <- data[, setdiff(names(data), c("X1ª_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$X1ª_eval)
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
library(randomForest)
# LOO
for (i in 1:nrow(X)) {
X_train <- X[-i, ]
Y_train <- Y[-i]
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500, mtry = floor(sqrt(ncol(X_train))))
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
mypred[i] <- as.character(pred_i)
}
# Convertir a factor con niveles iguales a Y
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusión
library(caret)
confusionMatrix(mypred, Y)
knitr::opts_chunk$set(echo = TRUE)
# Instala si no los tienes
#install.packages(c("readxl", "randomForest", "caret", "fastshap", "dplyr"))
# Cargar librerías
library(openxlsx)
library(randomForest)
library(caret)
library(fastshap)
library(dplyr)
# Leer Excel
data <- read.xlsx("datos_PCA.xlsx")
# 1. Preparar X e Y
Y <- as.factor(data$X1ª_eval)
X <- data %>%
select(-c('X1ª_eval', 'Mejor_resp')) %>%
select(where(is.numeric)) %>%
as.data.frame()
# Suponemos que tienes cargado el paquete randomForest y caret
# X = variables numéricas predictoras
data$ECOG <- as.numeric(data$ECOG)
X <- data[, setdiff(names(data), c("X1ª_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$X1ª_eval)
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
library(randomForest)
# LOO
for (i in 1:nrow(X)) {
X_train <- X[-i, ]
Y_train <- Y[-i]
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500, mtry = floor(sqrt(ncol(X_train))))
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
mypred[i] <- as.character(pred_i)
}
# Convertir a factor con niveles iguales a Y
mypred <- factor(mypred, levels = levels(Y))
# Matriz de confusión
library(caret)
confusionMatrix(mypred, Y)
# Suponemos que tienes cargado el paquete randomForest y caret
# X = variables numéricas predictoras
data$ECOG <- as.numeric(data$ECOG)
X <- data[, setdiff(names(data), c("X1ª_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$X1ª_eval)
# Vector para almacenar predicciones
mypred <- character(nrow(X))
names(mypred) <- rownames(X)
library(randomForest)
# LOO
for (i in 1:nrow(X)) {
X_train <- X[-i, ]
Y_train <- Y[-i]
rf_model <- randomForest(x = X_train, y = Y_train, ntree = 500, mtry = floor(sqrt(ncol(X_train))))
X_test <- X[i, , drop = FALSE]
pred_i <- predict(rf_model, newdata = X_test)
mypred[i] <- as.character(pred_i)
}
# Convertir a factor con niveles iguales a Y
mypred <- factor(mypred, levels = levels(Y))
confusionMatrix(mypred, Y)
knitr::opts_chunk$set(echo = TRUE)
# Instala si no los tienes
#install.packages(c("readxl", "randomForest", "caret", "fastshap", "dplyr"))
# Cargar librerías
library(openxlsx)
library(randomForest)
library(caret)
library(fastshap)
library(dplyr)
# Leer Excel
data <- read.xlsx("datos_PCA.xlsx")
# 1. Preparar X e Y
Y <- as.factor(data$X1ª_eval)
X <- data %>%
select(-c('X1ª_eval', 'Mejor_resp')) %>%
select(where(is.numeric)) %>%
as.data.frame()
library(dplyr)
library(caret)
library(randomForest)
# 1. Preparar X e Y
Y <- as.factor(data$X1ª_eval)
X <- data %>%
select(-c('X1ª_eval', 'Mejor_resp')) %>%
select(where(is.numeric)) %>%
as.data.frame()  # convertir a data.frame por si train lo necesita
library(openxlsx)
library(dplyr)
library(fastDummies)
library(ropls)
library(ggplot2)
library(ggrepel)
library(caret)
library(randomForest)
data <- read.xlsx("datos_PCA.xlsx")
data$ECOG <- as.numeric(data$ECOG)
X <- data[, setdiff(names(data), c("X1ª_eval", "Mejor_resp"))]
X <- X[, sapply(X, is.numeric)]
Y <- as.factor(data$X1ª_eval)
# Creamos un vector vacío para almacenar las predicciones
mypred <- character(nrow(X))  # o factor() si prefieres
names(mypred) <- rownames(X)
# LOO
for (i in 1:nrow(X)) {
# Creamos datos de entrenamiento sin la i-ésima fila
X_train <- X[-i, ]
Y_train <- Y[-i]
# Modelo PLS-DA con entrenamiento
pls_model <- opls(X_train, Y_train, predI = 2, orthoI = 0)
# Predicción de la muestra dejada fuera
X_test <- X[i, , drop = FALSE]
pred_i <- predict(pls_model, newdata = X_test)
# Guardamos la predicción
mypred[i] <- as.character(pred_i)
}
# Convertimos a factor con los mismos niveles que Y
mypred <- factor(mypred, levels = levels(Y))
# Confusion matrix
confusionMatrix(mypred, Y)
