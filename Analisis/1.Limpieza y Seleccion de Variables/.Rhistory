library(dplyr)
library(grid)
library(gridExtra)
library(tibble)
library(ggplot2)
library(ggcorrplot)
datos <- read_excel("datosLimpio.xlsx")
#datos$SII_pre = log(datos$SII_pre)
#datos$SII_1C = log(datos$SII_1C)
#datos$SII_1eval = log(datos$SII_1eval)
datos <- datos[, !(names(datos) %in% c("Mutation_status", "NLR1C_cut4","NLR1C_cut5","NLR2C_cut4or5"))]
tipos <- c(
rep("categorical", 1),  # 2C
rep("numerical", 12),    # 1N
rep("categorical", 1),  # 2C
rep("numerical", 1),    # 8N
rep("categorical", 1),  # 4C
rep("numerical", 2),    # 1N
rep("categorical", 1),  # 1C
rep("numerical", 22),   # 49N
rep("categorical", 4),  # 1C
rep("numerical", 1),    # 1N
rep("categorical", 5),  # 3C
rep("numerical", 49),   # 5C
)
tipos <- c(
rep("categorical", 1),  # 2C
rep("numerical", 12),    # 1N
rep("categorical", 1),  # 2C
rep("numerical", 1),    # 8N
rep("categorical", 1),  # 4C
rep("numerical", 2),    # 1N
rep("categorical", 1),  # 1C
rep("numerical", 22),   # 49N
rep("categorical", 4),  # 1C
rep("numerical", 1),    # 1N
rep("categorical", 5),  # 3C
rep("numerical", 49)   # 5C
)
# 2. Validar número de columnas
if(length(tipos) != ncol(datos)) {
stop("El número de tipos no coincide con el número de columnas en 'datos'")
}
# 3. Crear descripción
descDatos <- data.frame(
variable = colnames(datos),
tipo = tipos,
stringsAsFactors = FALSE
)
rownames(descDatos) <- descDatos$variable
# 4. Aplicar transformación de tipos a 'datos'
for (i in seq_along(tipos)) {
if (tipos[i] == "categorical") {
datos[[i]] <- as.factor(datos[[i]])  # o usa as.character() si prefieres
} else if (tipos[i] == "numerical") {
datos[[i]] <- as.numeric(datos[[i]])  # por si hay alguna variable mal leída
}
}
# Filtrar solo las variables categóricas desde descDatos
variables_categoricas <- descDatos$variable[descDatos$tipo == "categorical"]
# Crear lista vacía para almacenar los resultados
lista_tablas <- list()
# Loop para calcular porcentajes por variable
for (var in variables_categoricas) {
tabla <- prop.table(table(datos[[var]])) * 100  # Porcentaje
df_tabla <- data.frame(
Variable = var,
Categoria = names(tabla),
Porcentaje = round(as.numeric(tabla), 2),
row.names = NULL
)
lista_tablas[[var]] <- df_tabla
}
# Unir todas las tablas en un solo DataFrame
tabla_final <- do.call(rbind, lista_tablas)
# Mostrar tabla
print(tabla_final)
write.csv(tabla_final, "tabla_porcentajes_categoricas_features.csv", row.names = FALSE)
library(fastDummies)
# 1. Variables que NO quieres convertir a dummies
excluir_dummies <- c("Patient_ID", "Best_responde", "First_eval")
# 1. Lista de variables que TÚ quieres binarizar
mis_vars_a_binarizar <- c("Stage", "Histology", "Interruption_reason",
"Education", "Household","Sex","Elderly","ECOG",
"Treatment_interruption","Progression","Second_line_treatment",
"Exitus","PFS_censored","Statins","Histology_num",
"Stage_num")
# 2. Verifica que existen y son tipo 'character'
datos[mis_vars_a_binarizar] <- lapply(datos[mis_vars_a_binarizar], as.factor)
#  3. Crear columnas dummy solo de esas, sin tocar el resto
datos_dummy_df <- dummy_cols(
datos,
select_columns = mis_vars_a_binarizar,
remove_selected_columns = TRUE,
remove_first_dummy = FALSE
)
# 4. Comprobación visual
str(datos_dummy_df)
# Excluir las variables categóricas originales (que ya no están en dummy_df)
# Solo excluir variables tipo "ID", "Best_responde", "First_eval"
variables_excluir <- c("Patient_ID", "X1ª_eval", "Mejor_resp")
# Crear dataset solo numérico para PCA
datos_PCA_numerico <- datos_dummy_df[, !(names(datos_dummy_df) %in% variables_excluir)]
# PCA
res.pca <- PCA(datos_PCA_numerico, scale.unit = TRUE, graph = FALSE, ncp = 10)
# Visualizar eigenvalues
eig.val <- get_eigenvalue(res.pca)
VPmedio <- 100 * (1 / nrow(eig.val))
fviz_eig(res.pca, addlabels = TRUE) +
geom_hline(yintercept = VPmedio, linetype = 2, color = "red")
kable(eig.val[1:6,])
K = 6
res.pca <- PCA(datos_PCA_numerico, scale.unit = TRUE, graph = FALSE, ncp = K)
reticulate::repl_python()
K = 6
res.pca <- PCA(datos_PCA_numerico, scale.unit = TRUE, graph = FALSE, ncp = K)
# Gráfico T2 Hotelling
misScores = res.pca$ind$coord[,1:K]
miT2 = colSums(t(misScores**2)/eig.val[1:K,1])
I = nrow(datos_dummy_df)
F95 = K*(I**2 - 1)/(I*(I - K)) * qf(0.95, K, I-K)
F99 = K*(I**2 - 1)/(I*(I - K)) * qf(0.99, K, I-K)
plot(1:length(miT2), miT2, type = "p", xlab = "Variables", ylab = "T2")
abline(h = F95, col = "orange", lty = 2, lwd = 2)
abline(h = F99, col = "red3", lty = 2, lwd = 2)
# Gráfico T2 Hotelling
misScores = res.pca$ind$coord[,1:K]
miT2 = colSums(t(misScores**2)/eig.val[1:K,1])
I = nrow(datos_dummy_df)
F95 = K*(I**2 - 1)/(I*(I - K)) * qf(0.95, K, I-K)
F99 = K*(I**2 - 1)/(I*(I - K)) * qf(0.99, K, I-K)
plot(1:length(miT2), miT2, type = "p", xlab = "Variables", ylab = "T2")
abline(h = F95, col = "orange", lty = 2, lwd = 2)
abline(h = F99, col = "red3", lty = 2, lwd = 2)
anomalas = which(miT2 > F95)
anomalas
variables_colorear <- c("X1ª_eval", "Mejor_resp")
# Definir solo las combinaciones deseadas
componentes_deseadas <- list(c(1, 2), c(3, 4), c(5, 6))
for (variable_colorear in variables_colorear) {
colorear_factor <- factor(datos[[variable_colorear]])
# Iterar sobre las combinaciones específicas
for (componentes in componentes_deseadas) {
eje_x <- componentes[1]
eje_y <- componentes[2]
titulo <- paste0("PC", eje_x, " vs PC", eje_y, " - Coloreado por ", variable_colorear)
p <- fviz_pca_ind(res.pca, axes = c(eje_x, eje_y), geom = "point",
habillage = colorear_factor) +
coord_fixed() +
ggtitle(titulo)
print(p)
}
}
library(ggplot2)
# Extraer coordenadas y añadir info
coord_ind <- as.data.frame(res.pca$ind$coord)
coord_ind$Patient_ID <- datos$Patient_ID
coord_ind$X1a_eval <- factor(datos$`X1ª_eval`)  # ¡ojo con el nombre exacto de la columna!
# Componentes deseadas
componentes_deseadas <- list(c(1, 2), c(3, 4), c(5, 6))
for (componentes in componentes_deseadas) {
eje_x <- componentes[1]
eje_y <- componentes[2]
nombre_x <- paste0("Dim.", eje_x)
nombre_y <- paste0("Dim.", eje_y)
p <- ggplot(coord_ind, aes_string(x = nombre_x, y = nombre_y, color = "X1a_eval")) +
geom_text(aes(label = Patient_ID), size = 3) +
geom_hline(yintercept = 0, linetype = "dashed") +
geom_vline(xintercept = 0, linetype = "dashed") +
coord_fixed() +
ggtitle(paste0("PC", eje_x, " vs PC", eje_y, " - Etiquetado y coloreado por X1ª_eval")) +
theme_minimal()
print(p)
}
fviz_pca_var(res.pca, axes = c(1,2), repel = TRUE, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
select.var = list(contrib = 50))
fviz_pca_var(res.pca, axes = c(1,2), repel = TRUE, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
select.var = list(contrib = 50))
fviz_pca_var(res.pca, axes = c(1,2), repel = TRUE, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
select.var = list(contrib = 50))
fviz_pca_var(res.pca, axes = c(1,2), repel = TRUE, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
select.var = list(contrib = 50))
fviz_pca_var(res.pca, axes = c(1,2), repel = TRUE, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
select.var = list(contrib = 50))  # Solo muestra las 20 más importantes
fviz_pca_var(res.pca, axes = c(1,2), repel = TRUE, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
select.var = list(contrib = 50))
library(readxl)
library(FactoMineR)
library(factoextra)
library(knitr)
library(dplyr)
library(grid)
library(gridExtra)
library(tibble)
library(ggplot2)
library(ggcorrplot)
datos <- read.csv("datos_limpios.csv")
#datos$SII_pre = log(datos$SII_pre)
#datos$SII_1C = log(datos$SII_1C)
#datos$SII_1eval = log(datos$SII_1eval)
datos <- datos[, !(names(datos) %in% c("Mutation_status", "NLR1C_cut4","NLR1C_cut5","NLR2C_cut4or5"))]
# 1. Crear vector de tipos según tu nueva estructura
tipos <- c(
rep("categorical", 2),  # 2C
rep("numerical", 1),    # 1N
rep("categorical", 2),  # 2C
rep("numerical", 8),    # 8N
rep("categorical", 4),  # 4C
rep("numerical", 1),    # 1N
rep("categorical", 1),  # 1C
rep("numerical", 49),   # 49N
rep("categorical", 1),  # 1C
rep("numerical", 1),    # 1N
rep("categorical", 3),  # 3C
rep("numerical", 1),    # 1N
rep("categorical", 5)   # 5C
)
# 2. Validar número de columnas
if(length(tipos) != ncol(datos)) {
stop("El número de tipos no coincide con el número de columnas en 'datos'")
}
# 3. Crear descripción
descDatos <- data.frame(
variable = colnames(datos),
tipo = tipos,
stringsAsFactors = FALSE
)
rownames(descDatos) <- descDatos$variable
# 4. Aplicar transformación de tipos a 'datos'
for (i in seq_along(tipos)) {
if (tipos[i] == "categorical") {
datos[[i]] <- as.factor(datos[[i]])  # o usa as.character() si prefieres
} else if (tipos[i] == "numerical") {
datos[[i]] <- as.numeric(datos[[i]])  # por si hay alguna variable mal leída
}
}
# Filtrar solo las variables categóricas desde descDatos
variables_categoricas <- descDatos$variable[descDatos$tipo == "categorical"]
# Crear lista vacía para almacenar los resultados
lista_tablas <- list()
# Loop para calcular porcentajes por variable
for (var in variables_categoricas) {
tabla <- prop.table(table(datos[[var]])) * 100  # Porcentaje
df_tabla <- data.frame(
Variable = var,
Categoria = names(tabla),
Porcentaje = round(as.numeric(tabla), 2),
row.names = NULL
)
lista_tablas[[var]] <- df_tabla
}
# Unir todas las tablas en un solo DataFrame
tabla_final <- do.call(rbind, lista_tablas)
# Mostrar tabla
print(tabla_final)
write.csv(tabla_final, "tabla_porcentajes_categoricas.csv", row.names = FALSE)
library(fastDummies)
# 1. Variables que NO quieres convertir a dummies
excluir_dummies <- c("Patient_ID", "Best_responde", "First_eval")
# 1. Lista de variables que TÚ quieres binarizar
mis_vars_a_binarizar <- c("Stage", "Histology", "Interruption_reason",
"Education", "Household","Sex","Elderly","ECOG",
"Treatment_interruption","Progression","Second_line_treatment",
"Exitus","PFS_censored","Statins","Histology_num",
"Stage_num")
# 2. Verifica que existen y son tipo 'character'
datos[mis_vars_a_binarizar] <- lapply(datos[mis_vars_a_binarizar], as.factor)
#  3. Crear columnas dummy solo de esas, sin tocar el resto
datos_dummy_df <- dummy_cols(
datos,
select_columns = mis_vars_a_binarizar,
remove_selected_columns = TRUE,
remove_first_dummy = FALSE
)
# 4. Comprobación visual
str(datos_dummy_df)
# Excluir las variables categóricas originales (que ya no están en dummy_df)
# Solo excluir variables tipo "ID", "Best_responde", "First_eval"
variables_excluir <- c("Patient_ID", "X1ª_eval", "Mejor_resp")
# Crear dataset solo numérico para PCA
datos_PCA_numerico <- datos_dummy_df[, !(names(datos_dummy_df) %in% variables_excluir)]
# PCA
res.pca <- PCA(datos_PCA_numerico, scale.unit = TRUE, graph = FALSE, ncp = 10)
# Visualizar eigenvalues
eig.val <- get_eigenvalue(res.pca)
VPmedio <- 100 * (1 / nrow(eig.val))
fviz_eig(res.pca, addlabels = TRUE) +
geom_hline(yintercept = VPmedio, linetype = 2, color = "red")
kable(eig.val[1:6,])
K = 6
res.pca <- PCA(datos_PCA_numerico, scale.unit = TRUE, graph = FALSE, ncp = K)
# Gráfico T2 Hotelling
misScores = res.pca$ind$coord[,1:K]
miT2 = colSums(t(misScores**2)/eig.val[1:K,1])
I = nrow(datos_dummy_df)
F95 = K*(I**2 - 1)/(I*(I - K)) * qf(0.95, K, I-K)
F99 = K*(I**2 - 1)/(I*(I - K)) * qf(0.99, K, I-K)
plot(1:length(miT2), miT2, type = "p", xlab = "Variables", ylab = "T2")
abline(h = F95, col = "orange", lty = 2, lwd = 2)
abline(h = F99, col = "red3", lty = 2, lwd = 2)
anomalas = which(miT2 > F95)
anomalas
variables_colorear <- c("X1ª_eval", "Mejor_resp")
# Definir solo las combinaciones deseadas
componentes_deseadas <- list(c(1, 2), c(3, 4), c(5, 6))
for (variable_colorear in variables_colorear) {
colorear_factor <- factor(datos[[variable_colorear]])
# Iterar sobre las combinaciones específicas
for (componentes in componentes_deseadas) {
eje_x <- componentes[1]
eje_y <- componentes[2]
titulo <- paste0("PC", eje_x, " vs PC", eje_y, " - Coloreado por ", variable_colorear)
p <- fviz_pca_ind(res.pca, axes = c(eje_x, eje_y), geom = "point",
habillage = colorear_factor) +
coord_fixed() +
ggtitle(titulo)
print(p)
}
}
library(ggplot2)
# Extraer coordenadas y añadir info
coord_ind <- as.data.frame(res.pca$ind$coord)
coord_ind$Patient_ID <- datos$Patient_ID
coord_ind$X1a_eval <- factor(datos$`X1ª_eval`)  # ¡ojo con el nombre exacto de la columna!
# Componentes deseadas
componentes_deseadas <- list(c(1, 2), c(3, 4), c(5, 6))
for (componentes in componentes_deseadas) {
eje_x <- componentes[1]
eje_y <- componentes[2]
nombre_x <- paste0("Dim.", eje_x)
nombre_y <- paste0("Dim.", eje_y)
p <- ggplot(coord_ind, aes_string(x = nombre_x, y = nombre_y, color = "X1a_eval")) +
geom_text(aes(label = Patient_ID), size = 3) +
geom_hline(yintercept = 0, linetype = "dashed") +
geom_vline(xintercept = 0, linetype = "dashed") +
coord_fixed() +
ggtitle(paste0("PC", eje_x, " vs PC", eje_y, " - Etiquetado y coloreado por X1ª_eval")) +
theme_minimal()
print(p)
}
contribT2 = function (X, scores, loadings, eigenval, observ, cutoff = 2) {
# X is data matrix and must be centered (or centered and scaled if data were scaled)
misScoresNorm = t(t(scores**2) / eigenval)
misContrib = NULL
for (oo in observ) {
print(rownames(scores)[oo])
print(scores[oo,])
misPCs = which(as.numeric(misScoresNorm[oo,]) > cutoff)
lacontri = sapply(misPCs, function (cc) (scores[oo,cc]/eigenval[cc])*loadings[,cc]*X[oo,])
lacontri = rowSums((1*(sign(lacontri) == 1))*lacontri)
misContrib = cbind(misContrib, lacontri)
}
colnames(misContrib) = rownames(misScoresNorm[observ,])
return(misContrib)
}
# Recuperamos los datos utilizados en el modelo PCA, centrados y escalados
data_T = datos[,descDatos$tipo == "numerical"]
data_T = data_T[,setdiff(colnames(data_T), c("rating", "weight", "cups"))]
data_T = scale(datos_PCA_numerico, center = TRUE, scale = TRUE)
X = as.matrix(data_T)
# Calculamos los loadings a partir de las coordenadas de las variables
# ya que la librería FactoMineR nos devuelve los loadings ponderados
# por la importancia de cada componente principal.
misLoadings = sweep(res.pca$var$coord, 2, sqrt(res.pca$eig[1:K,1]), FUN="/")
# Calculamos las contribuciones
mycontrisT2 = contribT2(X = X, scores = misScores, loadings = misLoadings,
eigenval = eig.val[1:K,1], observ = which.max(miT2),
cutoff = 2)
par(mar = c(10,2.3,3,1))
barplot(mycontrisT2[,1],las=2, #cex.names = 0.5,
main= paste0("Observación: ", rownames(datos_PCA_numerico)[which.max(miT2)]))
# Obtener contribuciones como vector
contribuciones_obs24 <- as.vector(mycontrisT2)
# Asignar nombres de variables desde las filas (rownames)
names(contribuciones_obs24) <- rownames(mycontrisT2)
# Ordenar top 10 contribuciones
top_contribs <- sort(contribuciones_obs24, decreasing = TRUE)[1:10]
# Mostrar nombres y valores
print(round(top_contribs, 4))
# Barplot con nombres correctos
barplot(top_contribs, las = 2, col = "steelblue",
main = "Top 10 Contribuciones - Observación 24",
ylab = "Contribución relativa", cex.names = 0.8)
# 1. Detectar columnas con NA en X
vars_con_na <- colnames(X)[apply(X, 2, function(x) any(is.na(x)))]
# 2. Filtrar solo variables sin NA
vars_validas <- setdiff(colnames(X), vars_con_na)
# 3. Filtrar X y misLoadings para quedarnos solo con columnas válidas
X_limpio <- X[, vars_validas]
misLoadings_limpio <- misLoadings[vars_validas, ]
# 4. Verificar dimensiones compatibles
if (ncol(X_limpio) == nrow(misLoadings_limpio)) {
# 5. Recalcular error de reconstrucción y SCR
myE <- X_limpio - misScores %*% t(misLoadings_limpio)
mySCR <- rowSums(myE^2)
# 6. Graficar SCR válidos
idx_validos <- which(is.finite(mySCR))
if (length(idx_validos) > 0) {
plot(idx_validos, mySCR[idx_validos], type = "l",
main = "Distancia al modelo (SCR)",
ylab = "SCR", xlab = "Observaciones",
ylim = c(0, max(mySCR, na.rm = TRUE)))
# Opcional: límites Chi-cuadrado
g <- var(mySCR, na.rm = TRUE) / (2 * mean(mySCR, na.rm = TRUE))
h <- (2 * mean(mySCR, na.rm = TRUE)^2) / var(mySCR, na.rm = TRUE)
chi2lim <- g * qchisq(0.95, df = h)
chi2lim99 <- g * qchisq(0.99, df = h)
abline(h = chi2lim, col = "orange", lty = 2, lwd = 2)
abline(h = chi2lim99, col = "red3", lty = 2, lwd = 2)
} else {
message("No hay valores finitos de SCR para graficar.")
}
} else {
stop("Las dimensiones de X y loadings no coinciden tras limpiar.")
}
outliers_scr <- which(mySCR > chi2lim)
print(outliers_scr)
## 1. Calcular SCR evitando errores por NA
mySCR <- apply(myE, 1, function(x) sum(x^2, na.rm = TRUE))
## 2. Función corregida para calcular contribuciones a la SCR
ContriSCR = function(E, SCR) {
contribucion = matrix(NA, nrow = nrow(E), ncol = ncol(E))
rownames(contribucion) = rownames(E)
colnames(contribucion) = colnames(E)
for (j in 1:nrow(E)) {
eind <- E[j, ]
if (!is.na(SCR[j]) && SCR[j] != 0) {
signo <- sign(eind)
contri <- (signo * (eind^2) / SCR[j]) * 100
contribucion[j, ] <- contri
}
}
return(contribucion)
}
## 3. Calcular contribuciones
mycontris <- ContriSCR(E = myE, SCR = mySCR)
## 4. Eliminar la observación 7 que da error
if (nrow(mycontris) >= 7) {
mycontris <- mycontris[-7, , drop = FALSE]
}
## 5. Graficar la primera observación válida
barplot(mycontris[1, ], las = 2, cex.names = 0.7,
main = paste('Contribuciones a SCR para Obs.', rownames(mycontris)[1]))
# Extraer contribuciones de la obs 7
contrib_7 <- mycontris[7, ]
# Ordenar y seleccionar top 10
top_contribs <- sort(contrib_7, decreasing = TRUE)[1:10]
# Mostrar
barplot(top_contribs, las = 2, col = "steelblue",
main = "Top 10 Contribuciones a SCR - Obs. 7",
ylab = "Contribución (%)", cex.names = 0.8)
print(round(top_contribs, 2))
top_vars <- names(top_contribs)
datos_dummy_df[7, top_vars]
fviz_pca_var(res.pca, axes = c(1,2), repel = TRUE, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
select.var = list(contrib = 50))  # Solo muestra las 20 más importantes
fviz_pca_var(res.pca, axes = c(3,4), repel = TRUE, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
select.var = list(contrib = 25))  # Solo muestra las 20 más importantes
fviz_pca_var(res.pca, axes = c(5,6), repel = TRUE, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
select.var = list(contrib = 25))  # Solo muestra las 20 más importantes
panel.cor <- function(x, y, digits = 2, prefix = "", cex.cor, ...) {
par(usr = c(0, 1, 0, 1))
r <- cor(x, y, use = "complete.obs")  # Evitar errores por NA
txt <- format(c(r, 0.123456789), digits = digits)[1]
txt <- paste0(prefix, txt)
if (missing(cex.cor)) cex.cor <- 0.8 / strwidth(txt)
text(0.5, 0.5, txt, cex = cex.cor * abs(r))
}
# Variables corregidas que sí existen en tus datos
vars_cor <- c(
"SII_1eval", "SII_2C", "SII_1C",
"NLR_1eval", "NLR_2C", "NLR_1C",
"PLR_1C", "PNI_1eval", "Albumin_1eval",
"Progression_1", "Exitus_1", "PFS"
)
# Verifica que todas están presentes
vars_cor <- intersect(vars_cor, names(datos_dummy_df))
# Gráfico
pairs(datos_dummy_df[, vars_cor],
lower.panel = panel.cor,
pch = 20, col = "darkorange")
fviz_contrib(res.pca, choice = "var", axes = 1, top = 20)
# Obtener contribuciones
var_contrib <- get_pca_var(res.pca)$contrib
# Sumar contribuciones en PC1 y PC2
contrib_total <- rowSums(var_contrib[, c(1, 2)])
# Seleccionar top 20 variables con mayor contribución
top20_vars <- names(sort(contrib_total, decreasing = TRUE)[1:40])
fviz_pca_biplot(res.pca, axes = c(1,2), labelsize = 3,
label = "var", select.var = list(name = top20_vars),
repel = TRUE, col.ind = datos$Mejor_resp)
# Obtener contribuciones
var_contrib <- get_pca_var(res.pca)$contrib
# Sumar contribuciones en PC1 y PC2
contrib_total <- rowSums(var_contrib[, c(1, 2)])
# Seleccionar top 20 variables con mayor contribución
top20_vars <- names(sort(contrib_total, decreasing = TRUE)[1:40])
fviz_pca_biplot(res.pca, axes = c(1,2), labelsize = 3,
label = "var", select.var = list(name = top20_vars),
repel = TRUE, col.ind = datos$X1ª_eval)
fviz_pca_var(res.pca, axes = c(1,2), repel = TRUE, col.var = "contrib",
gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
select.var = list(contrib = 50))
